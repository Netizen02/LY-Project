{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Lambda, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import backend as K\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Function to load data from a space-separated .txt file\n",
        "def read_space_separated_txt(file_path):\n",
        "    data_list = []\n",
        "\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "        for line in lines:\n",
        "            # Split each line based on spaces and append to the list\n",
        "            line_values = line.strip().split(' ')\n",
        "            data_list.append(line_values)\n",
        "\n",
        "    return data_list\n",
        "\n",
        "# Load English data from a .txt file\n",
        "file_path_english = 'English_Chapter.txt'  # Replace with your actual file path for English data\n",
        "english_data = read_space_separated_txt(file_path_english)\n",
        "\n",
        "# Load French data from a .txt file\n",
        "file_path_french = 'French_Chapter (2).txt'  # Replace with your actual file path for French data\n",
        "french_data = read_space_separated_txt(file_path_french)\n",
        "\n",
        "# Tokenize and pad the input sequences\n",
        "vocab_size = 10000\n",
        "max_sequence_length = 20\n",
        "\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=vocab_size)\n",
        "tokenizer.fit_on_texts(english_data + french_data)\n",
        "\n",
        "english_sequences = tokenizer.texts_to_sequences(english_data)\n",
        "french_sequences = tokenizer.texts_to_sequences(french_data)\n",
        "\n",
        "english_sequences = tf.keras.preprocessing.sequence.pad_sequences(english_sequences, maxlen=max_sequence_length)\n",
        "french_sequences = tf.keras.preprocessing.sequence.pad_sequences(french_sequences, maxlen=max_sequence_length)\n",
        "\n",
        "# Labels (1 for similar, 0 for dissimilar)\n",
        "labels = np.concatenate([np.ones(len(english_sequences)), np.zeros(len(french_sequences))])\n",
        "\n",
        "# Combine English and French data for training\n",
        "X1_combined = np.concatenate([english_sequences, french_sequences])\n",
        "X2_combined = np.concatenate([english_sequences, french_sequences])\n",
        "\n",
        "# Split the combined data into training and validation sets\n",
        "X1_train, X1_val, X2_train, X2_val, y_train, y_val = train_test_split(\n",
        "    X1_combined, X2_combined, labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Siamese network architecture\n",
        "embedding_dim = 50\n",
        "lstm_units = 50\n",
        "\n",
        "# Shared LSTM layer\n",
        "shared_lstm = LSTM(lstm_units)\n",
        "\n",
        "# Input layers\n",
        "input_1 = Input(shape=(max_sequence_length,))\n",
        "input_2 = Input(shape=(max_sequence_length,))\n",
        "\n",
        "# Embedding layers\n",
        "embedding_layer = Embedding(vocab_size, embedding_dim)\n",
        "embedded_1 = embedding_layer(input_1)\n",
        "embedded_2 = embedding_layer(input_2)\n",
        "\n",
        "# Shared LSTM layer\n",
        "encoded_1 = shared_lstm(embedded_1)\n",
        "encoded_2 = shared_lstm(embedded_2)\n",
        "\n",
        "# Lambda layer to calculate Manhattan Distance\n",
        "distance = Lambda(lambda x: K.abs(x[0] - x[1]))([encoded_1, encoded_2])\n",
        "\n",
        "# Fully connected layer\n",
        "prediction = Dense(1, activation='sigmoid')(distance)\n",
        "\n",
        "# Create the Siamese model\n",
        "siamese_model = Model(inputs=[input_1, input_2], outputs=prediction)\n",
        "\n",
        "# Compile the model\n",
        "siamese_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the Siamese model\n",
        "siamese_model.fit([X1_train, X2_train], y_train, validation_data=([X1_val, X2_val], y_val), epochs=10, batch_size=16)\n",
        "\n",
        "# Use the trained model to predict similarity on unseen data\n",
        "# Replace the following with your actual test data\n",
        "test_text_english = [\"The quick brown fox\"]\n",
        "test_text_french = [\"Le renard brun rapide\"]\n",
        "\n",
        "test_seq_english = tokenizer.texts_to_sequences(test_text_english)\n",
        "test_seq_french = tokenizer.texts_to_sequences(test_text_french)\n",
        "\n",
        "test_seq_english = tf.keras.preprocessing.sequence.pad_sequences(test_seq_english, maxlen=max_sequence_length)\n",
        "test_seq_french = tf.keras.preprocessing.sequence.pad_sequences(test_seq_french, maxlen=max_sequence_length)\n",
        "\n",
        "# Print test sequences\n",
        "print(\"Test Sequence (English):\", test_seq_english)\n",
        "print(\"Test Sequence (French):\", test_seq_french)\n",
        "\n",
        "# Predict similarity\n",
        "prediction = siamese_model.predict([test_seq_english, test_seq_french])\n",
        "\n",
        "# Print predicted similarity\n",
        "print(\"Predicted similarity:\", prediction[0][0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uedWzTgV0Su5",
        "outputId": "c68fc914-62ed-4c6f-caf9-1a1610895e17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "5/5 [==============================] - 5s 262ms/step - loss: 0.6932 - accuracy: 0.5333 - val_loss: 0.6933 - val_accuracy: 0.3684\n",
            "Epoch 2/10\n",
            "5/5 [==============================] - 0s 35ms/step - loss: 0.6931 - accuracy: 0.5333 - val_loss: 0.6934 - val_accuracy: 0.3684\n",
            "Epoch 3/10\n",
            "5/5 [==============================] - 0s 37ms/step - loss: 0.6931 - accuracy: 0.5333 - val_loss: 0.6936 - val_accuracy: 0.3684\n",
            "Epoch 4/10\n",
            "5/5 [==============================] - 0s 46ms/step - loss: 0.6930 - accuracy: 0.5333 - val_loss: 0.6937 - val_accuracy: 0.3684\n",
            "Epoch 5/10\n",
            "5/5 [==============================] - 0s 73ms/step - loss: 0.6930 - accuracy: 0.5333 - val_loss: 0.6939 - val_accuracy: 0.3684\n",
            "Epoch 6/10\n",
            "5/5 [==============================] - 0s 60ms/step - loss: 0.6930 - accuracy: 0.5333 - val_loss: 0.6939 - val_accuracy: 0.3684\n",
            "Epoch 7/10\n",
            "5/5 [==============================] - 0s 59ms/step - loss: 0.6929 - accuracy: 0.5333 - val_loss: 0.6940 - val_accuracy: 0.3684\n",
            "Epoch 8/10\n",
            "5/5 [==============================] - 0s 61ms/step - loss: 0.6929 - accuracy: 0.5333 - val_loss: 0.6941 - val_accuracy: 0.3684\n",
            "Epoch 9/10\n",
            "5/5 [==============================] - 0s 52ms/step - loss: 0.6929 - accuracy: 0.5333 - val_loss: 0.6943 - val_accuracy: 0.3684\n",
            "Epoch 10/10\n",
            "5/5 [==============================] - 0s 52ms/step - loss: 0.6928 - accuracy: 0.5333 - val_loss: 0.6945 - val_accuracy: 0.3684\n",
            "Test Sequence (English): [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]]\n",
            "Test Sequence (French): [[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 17]]\n",
            "1/1 [==============================] - 1s 875ms/step\n",
            "Predicted similarity: 0.49845064\n"
          ]
        }
      ]
    }
  ]
}